{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "#print(\"current_dir: \" + current_dir)\n",
    "csv_path = os.path.join(current_dir, 'data', 'filmtv_movies.csv')\n",
    "#print(\"csv_path: \" + csv_path)\n",
    "\n",
    "csv_path = os.path.normpath(csv_path)\n",
    "\n",
    "dataset = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First dataset rows inspection\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and data type check\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values count\n",
    "dataset.nunique(axis=0, dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values count\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the columns 'genre', 'country' and 'directors' have very few missing values, I drop rows where at least one of these columns has a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'genre', 'country', or 'directors' have missing values\n",
    "dataset = dataset.dropna(subset=['genre', 'country', 'directors'])\n",
    "dataset.info()\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates check\n",
    "I have searched in the dataset for duplicates row. As it can be seen below, there are no duplicates in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates row search\n",
    "print(dataset.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'notes' column drop\n",
    "Since the 'notes' columns does not add relevant information about movies and has more than 20k missing values, I decided to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'notes' column\n",
    "dataset = dataset.drop(columns=['notes'])\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the selected columns to string type\n",
    "dataset['title'] = dataset['title'].astype(str)\n",
    "dataset['actors'] = dataset['actors'].astype(str)\n",
    "dataset['directors'] = dataset['directors'].astype(str)\n",
    "dataset['description'] = dataset['description'].astype(str)\n",
    "dataset['country'] = dataset['country'].astype(str)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Votes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics of vote counts\n",
    "dataset['total_votes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of vote counts\n",
    "sns.histplot(dataset['total_votes'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Number of Votes\")\n",
    "plt.xlabel(\"Number of Votes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dataset['total_votes'], bins=50, kde=False, color='blue')\n",
    "plt.title('Distribution of Total Votes')\n",
    "plt.xlabel('Total Votes')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.yscale('log')  # Optional: Use log scale for better visualization if data is skewed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=dataset, x='total_votes', y='avg_vote', alpha=0.5, color='blue')\n",
    "plt.title('Number of Votes vs Average Rating')\n",
    "plt.xlabel('Number of Votes')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xscale('log')  # Optional: Use log scale for better visibility of outliers\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by 'total_votes' and count the number of movies for each vote count\n",
    "votes_count = dataset['total_votes'].value_counts().sort_index()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(votes_count.index, votes_count.values, color='blue', width=1.0)\n",
    "plt.title('Number of Movies for Each Number of Votes')\n",
    "plt.xlabel('Number of Votes')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xlim(0, 100)  # Optional: Limit x-axis to a smaller range for better visibility\n",
    "plt.yscale('log')  # Optional: Use log scale if the distribution is heavily skewed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies ratings\n",
    "This dataset, for each movie, is taking into account:\n",
    "- average of all the votes;\n",
    "- public votes;\n",
    "- critics votes;\n",
    "- number of all the votes received.\n",
    "\n",
    "Since I want the final rating of a movie to be a single number and 'public_vote' and 'critics_vote' columns have many missing values, I decided to drop these columns and to base movies ratings on:\n",
    "- the average of all the votes received by the movie;\n",
    "- the number of all the votes received by the movie.\n",
    "\n",
    "'avg_vote' and 'total_votes' columns have not missing values. This has encouraged me on considering only these two columns for the \"weighted ratings\" even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'public_vote' and 'critics_vote' column\n",
    "dataset = dataset.drop(columns=['public_vote', 'critics_vote'])\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I stated before, the final rating column will represent a weighted rating based on the average and the number of votes received by each movie.\n",
    "\n",
    "The weighted rating will \"weight\" more movies with high number of total votes received. This way, the most reliable rating will be the ones with high number of votes. Insteead, movies with a low number of votes will be rated with a value closer to the average of all the votes in the dataset than its own average vote value. This means that movies with few votes will count less, since they are not so realiable.\n",
    "\n",
    "This rating system base its final votes using some parameters/constants:\n",
    "- `global_avg`: the global average of the 'avg_votes' between all the movies in the dataset;\n",
    "- `min_num_of_votes`: constant that represents the minimun number of votes a movie must have to be considered \"reliable\" (with respect on the 'avg_vote').\n",
    "\n",
    "I decided to set `min_num_of_votes` to 36 because the 75% of the movies in the dataset have 36 or fewer votes (see the \"Votes analysis\" section and the \"Distribution of Weighted Ratings for Different `min_num_of_votes` Values\" plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg = dataset['avg_vote'].mean()\n",
    "print(\"Global average: \", global_avg)\n",
    "\n",
    "min_num_of_votes = 36\n",
    "\n",
    "# Weighted rating definition\n",
    "def weighted_rating(x, m=min_num_of_votes, G=global_avg):\n",
    "    v = x['total_votes']  # Number of votes\n",
    "    R = x['avg_vote']   # Average rating\n",
    "    return (v / (v + m) * R) + (m / (v + m) * G)\n",
    "\n",
    "# Weigthed rating application to the dataset\n",
    "dataset['weighted_rating'] = dataset.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the weighted rating\n",
    "top_movies_weighted_discending = dataset.sort_values('weighted_rating', ascending=False)\n",
    "\n",
    "# Display the top 10 movies sorting by the weighted rating\n",
    "print(\"Discending ratings based on weights:\")\n",
    "print(top_movies_weighted_discending[['title', 'avg_vote', 'total_votes', 'weighted_rating']].head(10))\n",
    "\n",
    "# Sort by the weighted rating\n",
    "top_movies_weighted_ascending = dataset.sort_values('weighted_rating', ascending=True)\n",
    "\n",
    "# Display the top 10 movies sorting by the weighted rating\n",
    "print(\"Ascending ratings based on weights:\")\n",
    "print(top_movies_weighted_ascending[['title', 'avg_vote', 'total_votes', 'weighted_rating']].head(10))\n",
    "\n",
    "# Sort by the weighted rating\n",
    "top_movies_avg_discenging = dataset.sort_values('avg_vote', ascending=False)\n",
    "\n",
    "# Display the top 10 movies sorting by the weighted rating\n",
    "print(\"Discending ratings based on avg:\")\n",
    "print(top_movies_avg_discenging[['title', 'avg_vote', 'total_votes', 'weighted_rating']].head(10))\n",
    "\n",
    "# Sort by the weighted rating\n",
    "top_movies_avg_ascending = dataset.sort_values('avg_vote', ascending=True)\n",
    "\n",
    "# Display the top 10 movies sorting by the weighted rating\n",
    "print(\"Ascending ratings based on avg:\")\n",
    "print(top_movies_avg_ascending[['title', 'avg_vote', 'total_votes', 'weighted_rating']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Test different values of m (e.g., median, 75th percentile, 90th percentile)\n",
    "percentiles = [25, 50, 75, 90]  # Percentiles to test\n",
    "m_values = [np.percentile(dataset['total_votes'], p) for p in percentiles]\n",
    "\n",
    "# Create a new column for each weighted rating based on different m values\n",
    "for m_value in m_values:\n",
    "    dataset[f'weighted_rating_m_{m_value}'] = dataset.apply(weighted_rating, m=m_value, axis=1)\n",
    "\n",
    "# Plot the distribution of weighted ratings for different m values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Use a boxplot to compare the distribution of weighted ratings for each m value\n",
    "ratings_columns = [f'weighted_rating_m_{m_value}' for m_value in m_values]\n",
    "plt.boxplot([dataset[col] for col in ratings_columns], labels=[f'm = {m_value}' for m_value in m_values])\n",
    "\n",
    "plt.title('Distribution of Weighted Ratings for Different min_num_of_votes Values (m)')\n",
    "plt.ylabel('Weighted Rating')\n",
    "plt.xlabel('m Value (Percentile)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(dataset['avg_vote'])\n",
    "\n",
    "plt.title('Distribution of Average Votes (avg_vote)')\n",
    "plt.xlabel('Average Vote')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dataset = dataset.drop(columns=['weighted_rating_m_5.0',\n",
    "                                'weighted_rating_m_12.0',\n",
    "                                'weighted_rating_m_36.0',\n",
    "                                'weighted_rating_m_97.0'])\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Distribution of Weighted Ratings for Different `min_num_of_votes` Values\" plot comments considering also results :\n",
    "- `min_num_of_votes = 5`: ratings rely too much on movies with few votes (weighted votes results not so reliable indeed);\n",
    "- `min_num_of_votes = 12`: more balanced solution. Weighted ratings rely on movie with a higher number of votes;\n",
    "- `min_num_of_votes = 36`: another balanced solution. Weighted ratings rely on movie with a higher number of votes and are a well spreaded\n",
    "- `min_num_of_votes = 96`: exaggerated considering than 75% of the movies have less than 36 votes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by 'duration' and count the number of movies for each duration count\n",
    "votes_count = dataset['duration'].value_counts().sort_index()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(votes_count.index, votes_count.values, color='red', width=1.0)\n",
    "plt.title('Number of Movies for Each Duration')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xlim(0, 1000)  # Optional: Limit x-axis to a smaller range for better visibility\n",
    "plt.yscale('log')  # Optional: Use log scale if the distribution is heavily skewed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics of durations\n",
    "print(dataset['duration'].describe())\n",
    "\n",
    "print(dataset['duration'].quantile(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration normalisation\n",
    "\n",
    "Since 'duration' column has a very wide range and the distribution graph shows there are many outliers with a very long duration, this column need some type of normalisation. I will apply two different normalisation techniques, I will compare results and then decide which technique to apply definitevely.\n",
    "\n",
    "The techniques I decided to apply for the normalisation are:\n",
    "- \"log transformation\": it riduce outliers impact and restrict variables' interval;\n",
    "- \"robust scaler\": useful technique when there are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation application\n",
    "dataset['duration_log'] = dataset['duration'].apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Robust Scaler application\n",
    "scaler = RobustScaler()\n",
    "dataset['duration_robust'] = scaler.fit_transform(dataset[['duration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select movies by duration\n",
    "def select_movies_by_duration(min_duration, max_duration):\n",
    "    selected_movies = dataset[(dataset['duration'] >= min_duration) & (dataset['duration'] <= max_duration)]\n",
    "    return selected_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_by_duration = select_movies_by_duration(0, 42)\n",
    "\n",
    "# Visualizzare le prime righe dei film selezionati\n",
    "print(movies_by_duration[['title', 'duration', 'duration_log', 'duration_robust']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['duration_log'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['duration_robust'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset[['duration', 'duration_log', 'duration_robust']].head())\n",
    "\n",
    "\n",
    "# Compare results creating a graph\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Histograms of original, log-transformed and robust scaled duration\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(dataset['duration'], bins=50, color='blue', alpha=0.7, label='Original Duration')\n",
    "plt.hist(dataset['duration_log'], bins=50, color='green', alpha=0.7, label='Log Transformed Duration')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Duration Comparison (Original vs Log)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(dataset['duration'], bins=50, color='blue', alpha=0.7, label='Original Duration')\n",
    "plt.hist(dataset['duration_robust'], bins=50, color='red', alpha=0.7, label='Robust Scaled Duration')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Duration Comparison (Original vs Robust Scaler)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs creation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Log Transformation\n",
    "axes[0].hist(dataset['duration'].apply(lambda x: np.log(x + 1)), bins=50, color='red', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(\"Distribution of Log-Transformed Duration\")\n",
    "axes[0].set_xlabel('Log(Duration)')\n",
    "axes[0].set_ylabel('Number of Movies')\n",
    "axes[0].set_xscale('linear')\n",
    "\n",
    "# RobustScaler\n",
    "axes[1].hist(scaler.fit_transform(dataset[['duration']]), bins=50, color='blue', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(\"Distribution of Robust Scaled Duration\")\n",
    "axes[1].set_xlabel('Robust Scaled Duration')\n",
    "axes[1].set_ylabel('Number of Movies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since duration data are very asymmetric and with \"extreme\" outliers (movies whose duration is higher than 1000 minutes), the 'log transformation' normalisation is a very valid option for duration normalisation. After this step, duration will have a more uniform distribution and will not have outliers any more.\n",
    "\n",
    "Robust scaler option is not optimal in this case, since it does not treat the outlier issue as good as the other technique. In addition, the output of this normalisation procedure has negative values, too. As a consequence, the application of the 'duration_robust' column in the final recommendation model could result more complicated. For these reasons, I decided to definitevely apply the 'log transformation' technique and to drop the 'duration_robust' column, since it will not be useful in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'duration_robust' column\n",
    "dataset = dataset.drop(columns=['duration_robust'])\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# LabelEncoder initialisation\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# LabelEncoder application on 'genre' column\n",
    "dataset['genre_encoded'] = label_encoder.fit_transform(dataset['genre'])\n",
    "\n",
    "print(dataset[['genre', 'genre_encoded']].head())\n",
    "dataset.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numerical_columns = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = dataset[numerical_columns].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actors and director count (and extraction)\n",
    "I want to extract from the \"actors\" and \"directors\" columns the number of actors and directors in the dataset.\n",
    "Since these columns contain multiple names for each movie, I have to extract single names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"directors\" column\n",
    "directors = dataset['directors']\n",
    "\n",
    "# Drop missing values (if any)\n",
    "directors = directors.dropna()\n",
    "\n",
    "# Split comma-separated director names into a list (if applicable)\n",
    "director_list = directors.str.split(\",\").explode().str.strip()\n",
    "\n",
    "# Get unique director names\n",
    "unique_directors = director_list.unique()\n",
    "number_of_directors = len(unique_directors)\n",
    "print(\"Number of directors in the dataset: \", number_of_directors)\n",
    "\n",
    "# Print the list of unique directors\n",
    "# for index, director in enumerate(unique_directors):\n",
    "#     print(index, director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"actors\" column\n",
    "actors = dataset['actors']\n",
    "\n",
    "# Drop missing values (if any)\n",
    "actors = actors.dropna()\n",
    "\n",
    "# Split comma-separated actor names into a list (if applicable)\n",
    "actor_list = actors.str.split(\",\").explode().str.strip()\n",
    "\n",
    "# Get unique actor names\n",
    "unique_actors = actor_list.unique()\n",
    "number_of_actors = len(unique_actors)\n",
    "print(\"Number of actors in the dataset: \", number_of_actors)\n",
    "\n",
    "# Print the list of unique actors\n",
    "# for index, actor in enumerate(unique_actors):\n",
    "#     print(index, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"conuntry\" column\n",
    "countries = dataset['country']\n",
    "\n",
    "# Drop missing values (if any)\n",
    "countries = countries.dropna()\n",
    "\n",
    "# Split comma-separated contry names into a list (if applicable)\n",
    "country_list = countries.str.split(\",\").explode().str.strip()\n",
    "\n",
    "# Get unique countries\n",
    "unique_countries = country_list.unique()\n",
    "number_of_countries = len(unique_countries)\n",
    "print(\"Number of countries in the dataset: \", number_of_countries)\n",
    "\n",
    "# Print the list of unique countries\n",
    "for index, country in enumerate(unique_countries):\n",
    "    print(index, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies classification with respect to duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "durata = dataset['duration']\n",
    "\n",
    "# Drop missing values (if any)\n",
    "durata = durata.dropna()\n",
    "\n",
    "for minutes in durata:\n",
    "    if minutes < 1800:\n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "dataset['duration_category'] = pd.cut(durata, bins=[0, 60, 120, 180, float('inf')], \n",
    "                                 labels=['Short', 'Medium', 'Long', 'Epic'])\n",
    "\n",
    "print(dataset.head())\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_movies(dataset, genre=None, max_duration=None, actors=None, directors=None, start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    Filters the movies dataset based on user inputs.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (DataFrame): The dataset of movies.\n",
    "        genre (str): Filter movies by genre.\n",
    "        max_duration (int): Maximum duration of movies.\n",
    "        actors (str): Filter movies by actor name (partial or full match).\n",
    "        directors (str): Filter movies by director name (partial or full match).\n",
    "        start_year (int): Start year for filtering movies (inclusive).\n",
    "        end_year (int): End year for filtering movies (inclusive).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Filtered dataset based on the given parameters.\n",
    "    \"\"\"\n",
    "    filtered = dataset.copy()\n",
    "\n",
    "    # Filter by genre\n",
    "    if genre:\n",
    "        filtered = filtered[filtered['genre'].str.contains(genre, case=False, na=False)]\n",
    "\n",
    "    # Filter by duration\n",
    "    if max_duration is not None:\n",
    "        filtered = filtered[filtered['duration'] <= max_duration]\n",
    "\n",
    "    # Filter by actors\n",
    "    if actors:\n",
    "        filtered = filtered[filtered['actors'].str.contains(actors, case=False, na=False)]\n",
    "\n",
    "    # Filter by directors\n",
    "    if directors:\n",
    "        filtered = filtered[filtered['directors'].str.contains(directors, case=False, na=False)]\n",
    "\n",
    "    # Filter by year\n",
    "    if start_year is not None:\n",
    "        filtered = filtered[filtered['year'] >= start_year]\n",
    "    if end_year is not None:\n",
    "        filtered = filtered[filtered['year'] <= end_year]\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = filter_movies(dataset, actors=\"Jack Nicholson\", start_year=1980, end_year=2010, max_duration=120)\n",
    "selection.sort_values(by='weighted_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_movie(dataset, title=None):\n",
    "    filtered = dataset.copy()\n",
    "    if title:\n",
    "        filtered = filtered[filtered['title'].str.contains(title, case=False, na=False)]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = search_movie(dataset, title=\"The Shining\")\n",
    "movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processed dataset saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset in the 'data' folder\n",
    "output_path = \"C:/Users/beltr/OneDrive/Desktop/DDSE_PROJECT/MoviesRecommender/data/preprocessed_filmtv_movies.csv\"\n",
    "dataset.to_csv(output_path, index=False) # index=False: row indexes are not saved in the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDSE_project.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
